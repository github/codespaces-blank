local ConversationFlow = {}
local LLM_API = require("services.llm_api")
local CacheService = require("services.cache_service")
local SessionManager = require("modules.session_manager")
local config = require("config.config")

-- Function to process user input and generate a response
-- @param user_id: unique identifier for the user
-- @param user_input: the input provided by the user
-- @return: response generated by the LLM API
function ConversationFlow.process_input(user_id, user_input)
    -- Retrieve session context
    local session_context = SessionManager.get_context(user_id)

    -- Check cache for a similar request
    local cache_key = user_id .. ":" .. user_input
    local cached_response = CacheService.get(cache_key)
    if cached_response then
        return cached_response
    end

    -- Prepare payload for LLM API request
    local payload = {
        input = user_input,
        context = session_context
    }

    -- Make request to LLM API
    local response, err = LLM_API.request(config.llm_endpoint, payload, config.api_key)
    if not response then
        return "Error processing request: " .. err
    end

    -- Update session context with the response
    SessionManager.update_context(user_id, response.context)

    -- Cache the response
    CacheService.set(cache_key, response.output, config.cache_ttl)

    return response.output
end

return ConversationFlow
